{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lf6exlhP0z9"
   },
   "source": [
    "## Title Inverted Index Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUdFN2hMPqwU"
   },
   "source": [
    "# Importing from google storage & setup inverted indacies\n",
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!\n",
    "* don't forget to upload here \"title_inverted_index_gcp.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xfd0Ip4gPYzX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-b3d0  GCE       4                                       RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8dZjIUmsQE-E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "84S1Z4jYQhg8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a2lLysMLQlYX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Jan 14 07:46 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UNGnqVqdQyzj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-b3d0-m.c.ir-engine-gcp.internal:44991\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8dcf057520>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00SYA3AFRJcD"
   },
   "source": [
    "When creating the bucket we importing from the bucket \"wikidata20210801_preprocessed\" which holds the dump files relevant for our setup for the curpus and the building of the inverted indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1jJzP2TtROa9"
   },
   "outputs": [],
   "source": [
    "bucket_name = 'bucket_ir_engine' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if 'multistream' in b.name:\n",
    "        paths.append(full_path+b.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvk4ivCFRbQf"
   },
   "source": [
    "***GCP setup is complete!***\n",
    "If you got here without any errors you guys are wonderfull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERS8rusWRuGs"
   },
   "source": [
    "# *Building the Indexes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tIbH6hcR0GE"
   },
   "source": [
    "**Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index.\n",
    "In order to take entire corpus we will write \"parquetFile = spark.read.parquet(*paths)\", but first we will check on only one multistream file stored in \n",
    "varaible \"path\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hL-kFuNhS7ZT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "doc_text_pairs = parquetFile.select(\"text\",\"id\").rdd\n",
    "doc_anchor_pairs = parquetFile.select(\"anchor_text\",\"id\").rdd\n",
    "doc2title = parquetFile.select(\"id\", \"title\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving RDD in the form of dict to retrive titles to their doc id\n",
    "doc2title_dict = doc2title.collectAsMap()\n",
    "with open('/./home/dataproc/doc2title_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(doc2title_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4045403, 'Foster Air Force Base'), (4045413, 'Torino Palavela')]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc2title_dict.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4paWbdVlSq6K"
   },
   "source": [
    "* Counting the number of pages to make sure we on the right track (entire corpus should be more then 6M pages)\n",
    "* Checking how \"doc_title_pairs\" look like\n",
    "* Checking how \"doc_text_pairs\" look like\n",
    "* Checking how \"doc_anchor_pairs\" look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "moJ2uTfaSqXV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================================================> (121 + 3) / 124]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 6348910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "corpus_size = parquetFile.count()\n",
    "print(f\"corpus size: {corpus_size}\")\n",
    "# print(\"our RDDs at the begining:\")\n",
    "# doc_title_pairs.take(5)\n",
    "# doc_text_pairs.take(5)\n",
    "# doc_anchor_pairs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J4uWVAcTR6n"
   },
   "source": [
    "importing *_inverted_index_gcp modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R67Gife0TQJS",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_inverted_index_gcp.py\n",
      "body_inverted_index_gcp.py\n",
      "anchors_inverted_index_gcp.py\n",
      "inverted_index_gcp.py\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file storage_backend.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls title_inverted_index_gcp.py\n",
    "!ls body_inverted_index_gcp.py\n",
    "!ls anchors_inverted_index_gcp.py\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "043XtnKeTmoF"
   },
   "outputs": [],
   "source": [
    "# adding our python modules to the cluster\n",
    "sc.addFile(\"/home/dataproc/title_inverted_index_gcp.py\")\n",
    "sc.addFile(\"/home/dataproc/body_inverted_index_gcp.py\")\n",
    "sc.addFile(\"/home/dataproc/anchors_inverted_index_gcp.py\")\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zbHh_wACTtxF"
   },
   "outputs": [],
   "source": [
    "from title_inverted_index_gcp import InvertedIndex as Title_Inverted_Index\n",
    "from body_inverted_index_gcp import InvertedIndex as Body_Inverted_Index\n",
    "from anchors_inverted_index_gcp import InvertedIndex as Anchor_Inverted_Index\n",
    "from inverted_index_gcp import InvertedIndex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4m0-rMXUA5K"
   },
   "source": [
    "# Tokenization & More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oGKKTD99TzGL"
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "#tf score\n",
    "def word_count(text, _id):\n",
    "    \"\"\"\n",
    "    calculating the term freq, not including stopwords\n",
    "    Returns:\n",
    "      List of tuples: [(token, (doc Id, tf)), ...]\n",
    "    \"\"\"\n",
    "    #tokens = tokenize(text)\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())if token not in all_stopwords]\n",
    "    tok_counter = Counter()\n",
    "    tok_counter.update(tokens)\n",
    "    return [(token[0],(_id, token[1])) for token in tok_counter.items()]\n",
    "\n",
    "# sorting according to doc id\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    unsorted_pl = list(unsorted_pl)\n",
    "    return sorted(unsorted_pl, key = lambda tf: tf[0])\n",
    "\n",
    "def calculate_df(postings):\n",
    "    return postings.map(lambda x: (x[0], len(x[1])))\n",
    "\n",
    "def partition_postings_and_write_title(postings):\n",
    "    postings = postings.groupBy(lambda x: token2bucket_id(x[0]))\n",
    "    postings = postings.map(lambda x: Title_Inverted_Index.write_a_posting_list(x, bucket_name))\n",
    "    return postings\n",
    "\n",
    "def partition_postings_and_write_body(postings):\n",
    "    postings = postings.groupBy(lambda x: token2bucket_id(x[0]))\n",
    "    postings = postings.map(lambda x: Body_Inverted_Index.write_a_posting_list(x, bucket_name))\n",
    "    return postings\n",
    "\n",
    "def partition_postings_and_write_anchor(postings):\n",
    "    postings = postings.groupBy(lambda x: token2bucket_id(x[0]))\n",
    "    postings = postings.map(lambda x: Anchor_Inverted_Index.write_a_posting_list(x, bucket_name))\n",
    "    return postings\n",
    "\n",
    "def calculate_DocumentLengthXX(doc_id, text):\n",
    "    Length_Counter = 0\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())if token not in all_stopwords]\n",
    "    if(len(text) == 1):\n",
    "        return((doc_id,1))\n",
    "    for token in tokens:\n",
    "        Length_Counter +=1\n",
    "    return((doc_id,Length_Counter))  \n",
    "\n",
    "def Calculate_DocumentLength_AnchorXX(text,doc_id):\n",
    "    words_lst = []\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())if token not in all_stopwords]\n",
    "    for token in tokens:\n",
    "        words_lst.append(token)\n",
    "    return((doc_id,len(words_lst))) \n",
    "\n",
    "def calculate_TermTotal(postings):\n",
    "    def Term_Total_Count(postingList):\n",
    "        TermTotalSum= 0\n",
    "        for doc_id,tf in postingList:\n",
    "            TermTotalSum+=tf\n",
    "        return TermTotalSum   \n",
    "    return postings.mapValues(Term_Total_Count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG0fC6mvVGq0"
   },
   "source": [
    "# *** Let's get the TITLE Index Started! ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PUSnyuz3VMes"
   },
   "outputs": [],
   "source": [
    "# time the title index creation time\n",
    "title_start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vQvAMox-VRHU"
   },
   "outputs": [],
   "source": [
    "# title word count map\n",
    "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "#postings\n",
    "title_postings = word_counts_title.groupByKey().mapValues(reduce_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQzpqQRCXEBE"
   },
   "source": [
    "-> check how the title_postings filterd look like next to word_counts_title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nQ-oDlXRXDHs"
   },
   "outputs": [],
   "source": [
    "# print(\"At the begining:\")\n",
    "# word_counts_title.take(10)\n",
    "# print(\"After sorting:\")\n",
    "# title_postings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Hh_OZh1mXCpj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#calculating df\n",
    "w2df_title = calculate_df(title_postings)\n",
    "w2df_title_dict = w2df_title.collectAsMap()\n",
    "\n",
    "\n",
    "# #calculating DL\n",
    "DL_Array_Title = doc_title_pairs.map(lambda x:calculate_DocumentLengthXX(x[1],x[0]))\n",
    "DL_Dict_Title = DL_Array_Title.collectAsMap()\n",
    "\n",
    "\n",
    "#calculating TermTotal\n",
    "w2termstotal_Title = calculate_TermTotal(title_postings)\n",
    "dict_term_total_title = w2termstotal_Title.collectAsMap()\n",
    "\n",
    "\n",
    "# partition posting lists and write out\n",
    "posting_locs_Title = partition_postings_and_write_title(title_postings).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7au7JksVX6-e"
   },
   "source": [
    "-> check how the dictionaries (for the title index) that we created look like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE4Z4BRvY3kU"
   },
   "source": [
    "*** creating Title Inverted Index ***\n",
    "* posting_locs = posting locations dictionary\n",
    "* df = dictionary on the form {term: doc freq(how many docs it's in)}\n",
    "* DL = dictionary in form of {doc_id: doc_length}\n",
    "* term_total = dictionary in form of {term: total freq in corpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zVIJENgLaP8G"
   },
   "outputs": [],
   "source": [
    "super_posting_locs_title = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp/Title_data'):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)       \n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs_title[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://Title_index.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][132.9 MiB/132.9 MiB]                                                \n",
      "Operation completed over 1 objects/132.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted_title = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_title.posting_locs = super_posting_locs_title\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_title.df = w2df_title_dict\n",
    "#adding DL\n",
    "inverted_title.DL = DL_Dict_Title\n",
    "#adding Term Total\n",
    "inverted_title.term_total = dict_term_total_title\n",
    "\n",
    "\n",
    "# write the global stats out\n",
    "inverted_title.write_index('.', 'Title_index')\n",
    "\n",
    "\n",
    "#uploading to BUCKET\n",
    "index_src = \"Title_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/Indices/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yYKJZbzub3Ra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Inverted Index been created in 84.63939571380615 seconds\n"
     ]
    }
   ],
   "source": [
    "title_const_time = time() - title_start\n",
    "print(f\"Title Inverted Index been created in {title_const_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(doc_title_pairs.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5FdQvCfZX2qr"
   },
   "outputs": [],
   "source": [
    "# # title doc frequency per term\n",
    "# print(f\"w2df_title_dict {list(islice(w2df_title_dict.items(),5))}\")\n",
    "# # docs lengths\n",
    "# print(list(islice(DL_Dict_Title.items(), 5)))\n",
    "# # total freq for term in corpus titles\n",
    "# print(list(islice(dict_term_total_title.items(), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Let's get the BODY Index Started! *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time the body index creation time\n",
    "body_start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body word count map\n",
    "word_counts_body = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "#postings\n",
    "body_postings = word_counts_body.groupByKey().mapValues(reduce_word_counts)\n",
    "# filter out rare words < 50\n",
    "body_postings_filtered = body_postings.filter(lambda x: len(x[1])>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> check how the body_postings filterd look like next to word_counts_body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"At the begining:\")\n",
    "# word_counts_body.take(10)\n",
    "# print(\"After sorting:\")\n",
    "# body_postings.take(10)\n",
    "# body_postings_filtered.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#calculating df\n",
    "w2df_body = calculate_df(body_postings_filtered)\n",
    "w2df_body_dict = w2df_body.collectAsMap()\n",
    "\n",
    "#calculating DL (IMPORTANT)\n",
    "DL_Array_Body = doc_text_pairs.map(lambda x:calculate_DocumentLengthXX(x[1],x[0]))\n",
    "DL_Dict_Body = DL_Array_Body.collectAsMap()\n",
    "corpus_size = len(DL_Dict_Body)\n",
    "DL_sum = 0\n",
    "for  doc_id in DL_Dict_Body:\n",
    "    DL_sum += DL_Dict_Body[doc_id]\n",
    "avg_DL = DL_sum/corpus_size\n",
    "\n",
    "#calculating TermTotal (IMPORTANT)\n",
    "w2termstotal_body = calculate_TermTotal(body_postings_filtered)\n",
    "dict_term_total_body = w2termstotal_body.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#calculating idf\n",
    "idf_dict = {}\n",
    "for term, df_score in w2df_body_dict.items():\n",
    "    idf_dict[term] = math.log10((corpus_size + 1)/w2df_body_dict[term])\n",
    "\n",
    "# partition posting lists and write out\n",
    "posting_locs_Body = partition_postings_and_write_body(body_postings_filtered).collect() #DK - body_postings_filtered instead of body_postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> check how to w2df dictionary for the title index looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(doc2title_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*creating body Inverted Index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_posting_locs_body = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp/Body_data'):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)       \n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs_body[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://Body_index.pkl [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 76.2 MiB/ 76.2 MiB]                                                \n",
      "Operation completed over 1 objects/76.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted_body = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_body.posting_locs = super_posting_locs_body\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_body.df = w2df_body_dict\n",
    "#adding DL\n",
    "inverted_body.DL = DL_Dict_Body\n",
    "#adding Term Total\n",
    "inverted_body.term_total = dict_term_total_body\n",
    "\n",
    "# idf dict\n",
    "inverted_body.idf_dict = idf_dict\n",
    "#Avg_DL\n",
    "inverted_body.avg_DL = avg_DL\n",
    "inverted_title.avg_DL = avg_DL\n",
    "# corpus size\n",
    "inverted_body.corpus_size = corpus_size\n",
    "inverted_title.corpus_size = corpus_size\n",
    "\n",
    "# write the global stats out\n",
    "inverted_body.write_index('.', 'Body_index')\n",
    "\n",
    "\n",
    "#uploading to BUCKET\n",
    "index_src = \"Body_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/Indices/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body Inverted Index been created in 2935.8650636672974 seconds\n"
     ]
    }
   ],
   "source": [
    "body_const_time = time() - body_start\n",
    "print(f\"Body Inverted Index been created in {body_const_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Let's get the ANCHOR Index Started! *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time the anchor index creation time\n",
    "anchor_start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat\n",
    "flat_anchor_map = doc_anchor_pairs.flatMap(lambda x: x[0])\n",
    "distict_group_anchor = flat_anchor_map.distinct().groupByKey().mapValues(lambda x: \" \".join(x))\n",
    "\n",
    "# anchor word count map\n",
    "word_counts_anchor = distict_group_anchor.flatMap(lambda x: word_count(x[1], x[0]))\n",
    "#postings\n",
    "anchor_postings = word_counts_anchor.groupByKey().mapValues(reduce_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> check how the anchor_postings filterd look like next to word_counts_title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"At the begining:\")\n",
    "# word_counts_anchor.take(10)\n",
    "# print(\"After sorting:\")\n",
    "# anchor_postings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#calculating df\n",
    "w2df_anchor = calculate_df(anchor_postings)\n",
    "w2df_anchor_dict = w2df_anchor.collectAsMap()\n",
    "\n",
    "#calculating DL\n",
    "DL_Array_Anchor = distict_group_anchor.map(lambda x:Calculate_DocumentLength_AnchorXX(x[1],x[0]))\n",
    "DL_Dict_Anchor = DL_Array_Anchor.collectAsMap()\n",
    "\n",
    "#calculating TermTotal\n",
    "w2termstotal_anchor = calculate_TermTotal(anchor_postings)\n",
    "dict_term_total_anchor = w2termstotal_anchor.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "posting_locs_anchor = partition_postings_and_write_anchor(anchor_postings).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> check how to w2df dictionary for the anchor index looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(list(islice(w2df_anchor_dict.items(), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*creating anchor Inverted Index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_posting_locs_anchor = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp/Anchor_data'):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)       \n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs_anchor[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://Anchor_index.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "| [1 files][166.6 MiB/166.6 MiB]                                                \n",
      "Operation completed over 1 objects/166.6 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted_anchor = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_anchor.posting_locs = super_posting_locs_anchor\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_anchor.df = w2df_anchor_dict\n",
    "#adding DL\n",
    "inverted_anchor.DL = DL_Dict_Anchor\n",
    "#adding Term Total\n",
    "inverted_anchor.term_total = dict_term_total_anchor\n",
    "\n",
    "# write the global stats out\n",
    "inverted_anchor.write_index('.', 'Anchor_index')\n",
    "\n",
    "#uploading to BUCKET\n",
    "index_src = \"Anchor_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/Indices/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor Inverted Index been created in 1291.4913432598114 seconds\n"
     ]
    }
   ],
   "source": [
    "anchor_const_time = time() - anchor_start\n",
    "print(f\"Anchor Inverted Index been created in {anchor_const_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the cell below is to check term_total dictionary:\n",
    "##[('jurisdictions', 1210), ('concord', 416), ('crome', 7), ('adobes', 2), ('plasters', 15), ('slumber', 25), ('mylar', 20), ('grey-green', 1), ('spillover', 36), ('selber', 2), ('literary-critical', 2), ('vukmir', 1), ('ep-3e', 3), ('homocysteine', 10), ('earthy', 62), ('siena', 283), ('moonshot', 3), ('bruises', 42), ('candelabrum', 8), ('eclogae', 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inverted_body.term_total['jurisdictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2df_body_dict[\"jurisdictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title df: [('1970', 2419), ('bar', 1566)]\n",
      "title term_total: [('bar', 1574), ('intent', 72)]\n",
      "title DL: [(4045403, 4), (4045413, 2)]\n",
      "title avg_DL: 431.1623765339247\n",
      "title corpus_size: 6348910\n"
     ]
    }
   ],
   "source": [
    "print(f'title df: {list((inverted_title.df).items())[:2]}')\n",
    "print(f'title term_total: {list((inverted_title.term_total).items())[:2]}')\n",
    "print(f'title DL: {list((inverted_title.DL).items())[:2]}')\n",
    "print(f'title avg_DL: {inverted_title.avg_DL}')\n",
    "print(f'title corpus_size: {inverted_title.corpus_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
